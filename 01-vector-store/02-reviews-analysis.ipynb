{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Reviews Table Analysis Utility\n",
        "\n",
        "This notebook provides utility queries to analyze the `gold_customer_reviews` table in Databricks.\n",
        "\n",
        "**Purpose**: Validate table state, check data distributions, and verify data quality.\n",
        "\n",
        "**Last Generated**: Dec 25, 2025\n",
        "\n",
        "## Quick Reference\n",
        "\n",
        "| Metric | Expected Value |\n",
        "|--------|----------------|\n",
        "| Total Reviews | 5,000 |\n",
        "| Review Types | product_review (60%), return_feedback (20%), purchase_experience (20%) |\n",
        "| Rating Distribution | Positive-skewed (5-star: ~45%, 4-star: ~25%, etc.) |\n",
        "| Customer Segments | vip, premium, loyal, regular, new |\n",
        "| Product Categories | apparel, footwear, accessories |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Import Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --quiet pyyaml\n",
        "\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Add the src directory to Python path for clean imports\n",
        "sys.path.append('../src')\n",
        "\n",
        "from fashion_retail.config import load_config\n",
        "\n",
        "# Load configuration from project-level config.yaml\n",
        "config = load_config()\n",
        "\n",
        "CATALOG = config.catalog\n",
        "SCHEMA = config.schema\n",
        "TABLE_NAME = \"gold_customer_reviews\"\n",
        "FULL_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\"\n",
        "\n",
        "# Expected values for validation\n",
        "EXPECTED_TOTAL_REVIEWS = 5000\n",
        "EXPECTED_REVIEW_TYPES = {\n",
        "    \"product_review\": 0.60,\n",
        "    \"return_feedback\": 0.20,\n",
        "    \"purchase_experience\": 0.20\n",
        "}\n",
        "EXPECTED_RATING_DISTRIBUTION = {\n",
        "    5: 0.45,\n",
        "    4: 0.25,\n",
        "    3: 0.15,\n",
        "    2: 0.10,\n",
        "    1: 0.05\n",
        "}\n",
        "VALID_SEGMENTS = {\"vip\", \"premium\", \"loyal\", \"regular\", \"new\"}\n",
        "VALID_CATEGORIES = {\"apparel\", \"footwear\", \"accessories\"}\n",
        "\n",
        "print(f\"Configuration loaded from config.yaml\")\n",
        "print(f\"  Catalog: {CATALOG}\")\n",
        "print(f\"  Schema: {SCHEMA}\")\n",
        "print(f\"Target table: {FULL_TABLE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Table Schema & Row Count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display table schema with comments\n",
        "display(spark.sql(f\"DESCRIBE TABLE {FULL_TABLE_NAME}\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check total row count\n",
        "row_count_df = spark.sql(f\"SELECT COUNT(*) as total_reviews FROM {FULL_TABLE_NAME}\")\n",
        "total_reviews = row_count_df.collect()[0][\"total_reviews\"]\n",
        "\n",
        "print(f\"Total reviews: {total_reviews:,}\")\n",
        "print(f\"Expected: {EXPECTED_TOTAL_REVIEWS:,}\")\n",
        "print(f\"Status: {'‚úÖ MATCH' if total_reviews == EXPECTED_TOTAL_REVIEWS else '‚ùå MISMATCH'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Table History (Recent Updates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show recent table history (last 10 operations)\n",
        "display(spark.sql(f\"\"\"\n",
        "    DESCRIBE HISTORY {FULL_TABLE_NAME} \n",
        "    LIMIT 10\n",
        "\"\"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Review Type Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Review type distribution\n",
        "review_type_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        review_type,\n",
        "        COUNT(*) as count,\n",
        "        ROUND(COUNT(*) * 100.0 / {EXPECTED_TOTAL_REVIEWS}, 1) as actual_pct\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    GROUP BY review_type\n",
        "    ORDER BY count DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"Review Type Distribution:\")\n",
        "print(\"-\" * 50)\n",
        "for row in review_type_df.collect():\n",
        "    expected_pct = EXPECTED_REVIEW_TYPES.get(row[\"review_type\"], 0) * 100\n",
        "    status = \"‚úÖ\" if abs(float(row[\"actual_pct\"]) - expected_pct) < 2 else \"‚ö†Ô∏è\"\n",
        "    print(f\"{status} {row['review_type']:25} | {row['count']:,} ({row['actual_pct']}%) | Expected: {expected_pct}%\")\n",
        "\n",
        "display(review_type_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Rating Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rating distribution\n",
        "rating_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        rating,\n",
        "        COUNT(*) as count,\n",
        "        ROUND(COUNT(*) * 100.0 / {EXPECTED_TOTAL_REVIEWS}, 1) as actual_pct\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    GROUP BY rating\n",
        "    ORDER BY rating DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"Rating Distribution (should be positive-skewed):\")\n",
        "print(\"-\" * 50)\n",
        "star_symbols = {5: \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\", 4: \"‚≠ê‚≠ê‚≠ê‚≠ê\", 3: \"‚≠ê‚≠ê‚≠ê\", 2: \"‚≠ê‚≠ê\", 1: \"‚≠ê\"}\n",
        "for row in rating_df.collect():\n",
        "    expected_pct = EXPECTED_RATING_DISTRIBUTION.get(row[\"rating\"], 0) * 100\n",
        "    diff = abs(float(row[\"actual_pct\"]) - expected_pct)\n",
        "    status = \"‚úÖ\" if diff < 10 else \"‚ö†Ô∏è\"  # Allow some variance due to segment-based adjustments\n",
        "    print(f\"{status} {star_symbols[row['rating']]} ({row['rating']}) | {row['count']:,} ({row['actual_pct']}%) | Expected: ~{expected_pct}%\")\n",
        "\n",
        "display(rating_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Customer Segment Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer segment distribution\n",
        "segment_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        customer_segment,\n",
        "        COUNT(*) as count,\n",
        "        ROUND(COUNT(*) * 100.0 / {EXPECTED_TOTAL_REVIEWS}, 1) as pct\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    GROUP BY customer_segment\n",
        "    ORDER BY count DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"Customer Segment Distribution:\")\n",
        "print(\"-\" * 50)\n",
        "found_segments = set()\n",
        "for row in segment_df.collect():\n",
        "    segment = row[\"customer_segment\"]\n",
        "    found_segments.add(segment)\n",
        "    status = \"‚úÖ\" if segment in VALID_SEGMENTS else \"‚ùå UNEXPECTED\"\n",
        "    print(f\"{status} {segment:15} | {row['count']:,} reviews ({row['pct']}%)\")\n",
        "\n",
        "# Check for missing segments\n",
        "missing = VALID_SEGMENTS - found_segments\n",
        "if missing:\n",
        "    print(f\"\\n‚ö†Ô∏è Missing segments: {missing}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ All expected segments present\")\n",
        "\n",
        "display(segment_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Product Category Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Product category distribution\n",
        "category_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        product_category,\n",
        "        COUNT(*) as count,\n",
        "        ROUND(COUNT(*) * 100.0 / {EXPECTED_TOTAL_REVIEWS}, 1) as pct\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    GROUP BY product_category\n",
        "    ORDER BY count DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"Product Category Distribution:\")\n",
        "print(\"-\" * 50)\n",
        "found_categories = set()\n",
        "for row in category_df.collect():\n",
        "    category = row[\"product_category\"]\n",
        "    if category:  # Can be NULL for purchase_experience reviews\n",
        "        found_categories.add(category)\n",
        "    status = \"‚úÖ\" if category in VALID_CATEGORIES or category is None else \"‚ùå UNEXPECTED\"\n",
        "    cat_display = category if category else \"(NULL - purchase_experience)\"\n",
        "    print(f\"{status} {cat_display:25} | {row['count']:,} reviews ({row['pct']}%)\")\n",
        "\n",
        "display(category_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Date Range & Temporal Coverage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Date range analysis\n",
        "date_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        MIN(review_date) as earliest_review,\n",
        "        MAX(review_date) as latest_review,\n",
        "        DATEDIFF(MAX(review_date), MIN(review_date)) as days_covered,\n",
        "        COUNT(DISTINCT review_date) as unique_dates\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "\"\"\")\n",
        "\n",
        "date_row = date_df.collect()[0]\n",
        "print(\"Date Range Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Earliest Review: {date_row['earliest_review']}\")\n",
        "print(f\"Latest Review:   {date_row['latest_review']}\")\n",
        "print(f\"Days Covered:    {date_row['days_covered']} days\")\n",
        "print(f\"Unique Dates:    {date_row['unique_dates']}\")\n",
        "\n",
        "display(date_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Sample Data Quality Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample reviews with full content preview\n",
        "sample_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        review_id,\n",
        "        rating,\n",
        "        review_type,\n",
        "        review_title,\n",
        "        SUBSTRING(review_text, 1, 200) as review_preview,\n",
        "        customer_segment,\n",
        "        product_category,\n",
        "        product_brand,\n",
        "        sentiment_score,\n",
        "        word_count\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    ORDER BY review_date DESC\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "\n",
        "display(sample_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Sentiment & Content Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment score distribution by rating\n",
        "sentiment_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        rating,\n",
        "        ROUND(AVG(sentiment_score), 2) as avg_sentiment,\n",
        "        ROUND(MIN(sentiment_score), 2) as min_sentiment,\n",
        "        ROUND(MAX(sentiment_score), 2) as max_sentiment,\n",
        "        ROUND(AVG(word_count), 0) as avg_word_count\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    GROUP BY rating\n",
        "    ORDER BY rating DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"Sentiment Score by Rating (should correlate with star rating):\")\n",
        "print(\"-\" * 70)\n",
        "for row in sentiment_df.collect():\n",
        "    print(f\"Rating {row['rating']}: Avg Sentiment = {row['avg_sentiment']:>5} | \"\n",
        "          f\"Range: [{row['min_sentiment']}, {row['max_sentiment']}] | \"\n",
        "          f\"Avg Words: {row['avg_word_count']}\")\n",
        "\n",
        "display(sentiment_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topic mentions analysis\n",
        "topics_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        SUM(CASE WHEN mentions_sizing THEN 1 ELSE 0 END) as sizing_mentions,\n",
        "        SUM(CASE WHEN mentions_quality THEN 1 ELSE 0 END) as quality_mentions,\n",
        "        SUM(CASE WHEN mentions_delivery THEN 1 ELSE 0 END) as delivery_mentions,\n",
        "        SUM(CASE WHEN mentions_price THEN 1 ELSE 0 END) as price_mentions,\n",
        "        SUM(CASE WHEN mentions_comfort THEN 1 ELSE 0 END) as comfort_mentions,\n",
        "        SUM(CASE WHEN has_recommendation THEN 1 ELSE 0 END) as has_recommendation\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "\"\"\")\n",
        "\n",
        "topics_row = topics_df.collect()[0]\n",
        "print(\"Topic Mentions in Reviews:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"  Sizing/Fit:     {topics_row['sizing_mentions']:,} reviews\")\n",
        "print(f\"  Quality:        {topics_row['quality_mentions']:,} reviews\")\n",
        "print(f\"  Delivery:       {topics_row['delivery_mentions']:,} reviews\")\n",
        "print(f\"  Price/Value:    {topics_row['price_mentions']:,} reviews\")\n",
        "print(f\"  Comfort:        {topics_row['comfort_mentions']:,} reviews\")\n",
        "print(f\"  Recommendations: {topics_row['has_recommendation']:,} reviews\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Data Quality Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive data quality summary\n",
        "quality_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        -- Null checks\n",
        "        SUM(CASE WHEN review_id IS NULL THEN 1 ELSE 0 END) as null_review_ids,\n",
        "        SUM(CASE WHEN customer_key IS NULL THEN 1 ELSE 0 END) as null_customer_keys,\n",
        "        SUM(CASE WHEN review_text IS NULL OR LENGTH(review_text) = 0 THEN 1 ELSE 0 END) as empty_reviews,\n",
        "        SUM(CASE WHEN rating IS NULL OR rating < 1 OR rating > 5 THEN 1 ELSE 0 END) as invalid_ratings,\n",
        "        \n",
        "        -- FK validation counts (these should be checked against dimension tables)\n",
        "        COUNT(DISTINCT customer_key) as unique_customers,\n",
        "        COUNT(DISTINCT product_key) as unique_products,\n",
        "        \n",
        "        -- Content stats\n",
        "        ROUND(AVG(word_count), 1) as avg_word_count,\n",
        "        MIN(word_count) as min_word_count,\n",
        "        MAX(word_count) as max_word_count\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "\"\"\")\n",
        "\n",
        "qc = quality_df.collect()[0]\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA QUALITY SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "checks_passed = True\n",
        "\n",
        "# Null checks\n",
        "print(\"\\nüìã Null/Empty Checks:\")\n",
        "if qc['null_review_ids'] == 0:\n",
        "    print(\"  ‚úÖ No null review_ids\")\n",
        "else:\n",
        "    print(f\"  ‚ùå {qc['null_review_ids']} null review_ids\")\n",
        "    checks_passed = False\n",
        "\n",
        "if qc['null_customer_keys'] == 0:\n",
        "    print(\"  ‚úÖ No null customer_keys\")\n",
        "else:\n",
        "    print(f\"  ‚ùå {qc['null_customer_keys']} null customer_keys\")\n",
        "    checks_passed = False\n",
        "    \n",
        "if qc['empty_reviews'] == 0:\n",
        "    print(\"  ‚úÖ No empty review texts\")\n",
        "else:\n",
        "    print(f\"  ‚ùå {qc['empty_reviews']} empty review texts\")\n",
        "    checks_passed = False\n",
        "\n",
        "if qc['invalid_ratings'] == 0:\n",
        "    print(\"  ‚úÖ All ratings valid (1-5)\")\n",
        "else:\n",
        "    print(f\"  ‚ùå {qc['invalid_ratings']} invalid ratings\")\n",
        "    checks_passed = False\n",
        "\n",
        "# Stats\n",
        "print(f\"\\nüìä Content Statistics:\")\n",
        "print(f\"  Unique customers: {qc['unique_customers']:,}\")\n",
        "print(f\"  Unique products:  {qc['unique_products']:,}\")\n",
        "print(f\"  Avg word count:   {qc['avg_word_count']}\")\n",
        "print(f\"  Word count range: [{qc['min_word_count']}, {qc['max_word_count']}]\")\n",
        "\n",
        "# Final verdict\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if checks_passed:\n",
        "    print(\"‚úÖ ALL DATA QUALITY CHECKS PASSED\")\n",
        "else:\n",
        "    print(\"‚ùå SOME DATA QUALITY CHECKS FAILED - Review issues above\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
