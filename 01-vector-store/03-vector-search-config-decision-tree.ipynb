{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector Search Configuration Decision Tree\n",
        "\n",
        "**Purpose**: Analyze your data to make two key decisions for Databricks Vector Search:\n",
        "\n",
        "1. **Embedding Strategy** (Sections 1-10): Full text vs. chunking\n",
        "2. **Filter Column Selection** (Sections 11-15): Which columns to index for filtering\n",
        "\n",
        "---\n",
        "\n",
        "# Part 1: Embedding Strategy (Chunking Decision)\n",
        "\n",
        "## Decision Framework\n",
        "\n",
        "```\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚  Analyze Your Text Data         â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                    â”‚\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚  Avg Token Count per Document   â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                    â”‚\n",
        "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "              â”‚                     â”‚                     â”‚\n",
        "      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
        "      â”‚   < 256       â”‚     â”‚  256 - 512    â”‚     â”‚    > 512      â”‚\n",
        "      â”‚   tokens      â”‚     â”‚   tokens      â”‚     â”‚   tokens      â”‚\n",
        "      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "              â”‚                     â”‚                     â”‚\n",
        "              â–¼                     â–¼                     â–¼\n",
        "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "      â”‚ EMBED FULL    â”‚     â”‚ EMBED FULL    â”‚     â”‚ CHUNK TEXT    â”‚\n",
        "      â”‚ TEXT          â”‚     â”‚ (monitor      â”‚     â”‚ Required      â”‚\n",
        "      â”‚ âœ… Optimal    â”‚     â”‚ truncation)   â”‚     â”‚               â”‚\n",
        "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Key Thresholds\n",
        "\n",
        "| Embedding Model | Max Tokens | Safe Zone | Chunking Recommended |\n",
        "|-----------------|------------|-----------|----------------------|\n",
        "| `databricks-bge-large-en` | 512 | < 400 | > 512 |\n",
        "| `databricks-gte-large-en` | 512 | < 400 | > 512 |\n",
        "| OpenAI `text-embedding-3-small` | 8191 | < 6000 | > 8000 |\n",
        "| Cohere `embed-english-v3` | 512 | < 400 | > 512 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update these for your table\n",
        "CATALOG = \"juan_use1_catalog\"\n",
        "SCHEMA = \"retail\"\n",
        "TABLE_NAME = \"gold_customer_reviews\"\n",
        "TEXT_COLUMN = \"review_text\"  # Column to be embedded\n",
        "\n",
        "FULL_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\"\n",
        "\n",
        "# Embedding model constraints (Databricks BGE-Large-EN)\n",
        "EMBEDDING_MODEL = \"databricks-bge-large-en\"\n",
        "MAX_TOKENS = 512           # Model's maximum token limit\n",
        "SAFE_TOKEN_THRESHOLD = 400 # Leave buffer for special tokens\n",
        "CHARS_PER_TOKEN = 4        # Approximate (English text average)\n",
        "\n",
        "# Chunking parameters (if needed)\n",
        "CHUNK_SIZE = 400           # Tokens per chunk\n",
        "CHUNK_OVERLAP = 50         # Token overlap between chunks\n",
        "\n",
        "print(f\"Analyzing: {FULL_TABLE_NAME}\")\n",
        "print(f\"Text Column: {TEXT_COLUMN}\")\n",
        "print(f\"Embedding Model: {EMBEDDING_MODEL} (max {MAX_TOKENS} tokens)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Length Analysis (Characters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze character length distribution\n",
        "char_stats_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_documents,\n",
        "        MIN(LENGTH({TEXT_COLUMN})) as min_chars,\n",
        "        MAX(LENGTH({TEXT_COLUMN})) as max_chars,\n",
        "        ROUND(AVG(LENGTH({TEXT_COLUMN})), 0) as avg_chars,\n",
        "        PERCENTILE(LENGTH({TEXT_COLUMN}), 0.25) as p25_chars,\n",
        "        PERCENTILE(LENGTH({TEXT_COLUMN}), 0.50) as median_chars,\n",
        "        PERCENTILE(LENGTH({TEXT_COLUMN}), 0.75) as p75_chars,\n",
        "        PERCENTILE(LENGTH({TEXT_COLUMN}), 0.90) as p90_chars,\n",
        "        PERCENTILE(LENGTH({TEXT_COLUMN}), 0.95) as p95_chars,\n",
        "        PERCENTILE(LENGTH({TEXT_COLUMN}), 0.99) as p99_chars\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    WHERE {TEXT_COLUMN} IS NOT NULL\n",
        "\"\"\")\n",
        "\n",
        "stats = char_stats_df.collect()[0]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHARACTER LENGTH DISTRIBUTION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTotal Documents: {stats['total_documents']:,}\")\n",
        "print(f\"\\nBasic Statistics:\")\n",
        "print(f\"  Min:    {int(stats['min_chars']):,} chars\")\n",
        "print(f\"  Max:    {int(stats['max_chars']):,} chars\")\n",
        "print(f\"  Avg:    {int(stats['avg_chars']):,} chars\")\n",
        "print(f\"  Median: {int(stats['median_chars']):,} chars\")\n",
        "print(f\"\\nPercentile Distribution:\")\n",
        "print(f\"  P25:  {int(stats['p25_chars']):,} chars\")\n",
        "print(f\"  P50:  {int(stats['median_chars']):,} chars\")\n",
        "print(f\"  P75:  {int(stats['p75_chars']):,} chars\")\n",
        "print(f\"  P90:  {int(stats['p90_chars']):,} chars\")\n",
        "print(f\"  P95:  {int(stats['p95_chars']):,} chars\")\n",
        "print(f\"  P99:  {int(stats['p99_chars']):,} chars\")\n",
        "\n",
        "display(char_stats_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Token Estimation\n",
        "\n",
        "**Rule of Thumb**: For English text, approximately 1 token â‰ˆ 4 characters (or ~0.75 words)\n",
        "\n",
        "This is an approximation. For exact counts, use a tokenizer like `tiktoken` or the model's native tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estimate token counts from character lengths\n",
        "token_stats_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        -- Character stats\n",
        "        LENGTH({TEXT_COLUMN}) as char_count,\n",
        "        -- Estimated token count (chars / 4)\n",
        "        ROUND(LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN}, 0) as est_tokens,\n",
        "        -- Word count for reference\n",
        "        SIZE(SPLIT({TEXT_COLUMN}, ' ')) as word_count\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    WHERE {TEXT_COLUMN} IS NOT NULL\n",
        "\"\"\")\n",
        "\n",
        "# Calculate token distribution\n",
        "token_agg_df = token_stats_df.selectExpr(\n",
        "    \"COUNT(*) as total_docs\",\n",
        "    \"MIN(est_tokens) as min_tokens\",\n",
        "    \"MAX(est_tokens) as max_tokens\",\n",
        "    \"ROUND(AVG(est_tokens), 0) as avg_tokens\",\n",
        "    \"PERCENTILE(est_tokens, 0.50) as median_tokens\",\n",
        "    \"PERCENTILE(est_tokens, 0.90) as p90_tokens\",\n",
        "    \"PERCENTILE(est_tokens, 0.95) as p95_tokens\",\n",
        "    \"PERCENTILE(est_tokens, 0.99) as p99_tokens\",\n",
        "    f\"SUM(CASE WHEN est_tokens > {MAX_TOKENS} THEN 1 ELSE 0 END) as exceeds_max\",\n",
        "    f\"SUM(CASE WHEN est_tokens > {SAFE_TOKEN_THRESHOLD} THEN 1 ELSE 0 END) as exceeds_safe\"\n",
        ")\n",
        "\n",
        "token_stats = token_agg_df.collect()[0]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ESTIMATED TOKEN DISTRIBUTION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nToken Statistics (estimated at {CHARS_PER_TOKEN} chars/token):\")\n",
        "print(f\"  Min:    {int(token_stats['min_tokens']):,} tokens\")\n",
        "print(f\"  Max:    {int(token_stats['max_tokens']):,} tokens\")\n",
        "print(f\"  Avg:    {int(token_stats['avg_tokens']):,} tokens\")\n",
        "print(f\"  Median: {int(token_stats['median_tokens']):,} tokens\")\n",
        "print(f\"  P90:    {int(token_stats['p90_tokens']):,} tokens\")\n",
        "print(f\"  P95:    {int(token_stats['p95_tokens']):,} tokens\")\n",
        "print(f\"  P99:    {int(token_stats['p99_tokens']):,} tokens\")\n",
        "\n",
        "print(f\"\\nâš ï¸  Model Limit Analysis ({EMBEDDING_MODEL}):\")\n",
        "print(f\"  Documents exceeding {MAX_TOKENS} tokens (MAX):  {int(token_stats['exceeds_max']):,}\")\n",
        "print(f\"  Documents exceeding {SAFE_TOKEN_THRESHOLD} tokens (SAFE): {int(token_stats['exceeds_safe']):,}\")\n",
        "\n",
        "exceeds_pct = (token_stats['exceeds_max'] / token_stats['total_docs']) * 100\n",
        "print(f\"\\n  Truncation Risk: {exceeds_pct:.2f}% of documents\")\n",
        "\n",
        "display(token_agg_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Token Distribution Histogram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create token buckets for visualization\n",
        "bucket_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        CASE \n",
        "            WHEN LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN} < 50 THEN '0-50'\n",
        "            WHEN LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN} < 100 THEN '50-100'\n",
        "            WHEN LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN} < 150 THEN '100-150'\n",
        "            WHEN LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN} < 200 THEN '150-200'\n",
        "            WHEN LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN} < 256 THEN '200-256'\n",
        "            WHEN LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN} < 400 THEN '256-400 (SAFE)'\n",
        "            WHEN LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN} < 512 THEN '400-512 (CAUTION)'\n",
        "            ELSE '512+ (TRUNCATED)'\n",
        "        END as token_bucket,\n",
        "        COUNT(*) as doc_count\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    WHERE {TEXT_COLUMN} IS NOT NULL\n",
        "    GROUP BY 1\n",
        "    ORDER BY \n",
        "        CASE \n",
        "            WHEN token_bucket = '0-50' THEN 1\n",
        "            WHEN token_bucket = '50-100' THEN 2\n",
        "            WHEN token_bucket = '100-150' THEN 3\n",
        "            WHEN token_bucket = '150-200' THEN 4\n",
        "            WHEN token_bucket = '200-256' THEN 5\n",
        "            WHEN token_bucket = '256-400 (SAFE)' THEN 6\n",
        "            WHEN token_bucket = '400-512 (CAUTION)' THEN 7\n",
        "            ELSE 8\n",
        "        END\n",
        "\"\"\")\n",
        "\n",
        "print(\"Token Distribution by Bucket:\")\n",
        "print(\"-\" * 50)\n",
        "for row in bucket_df.collect():\n",
        "    bar_length = int(row['doc_count'] / 100)  # Scale for display\n",
        "    bar = \"â–ˆ\" * bar_length\n",
        "    status = \"âœ…\" if \"TRUNCATED\" not in row['token_bucket'] else \"âŒ\"\n",
        "    print(f\"{status} {row['token_bucket']:20} | {row['doc_count']:>6,} | {bar}\")\n",
        "\n",
        "display(bucket_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Decision Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Engine - Determine embedding strategy\n",
        "def make_decision(stats, token_stats):\n",
        "    \"\"\"\n",
        "    Decision tree for embedding strategy based on text analysis.\n",
        "    \n",
        "    Returns: (strategy, confidence, reasons)\n",
        "    \"\"\"\n",
        "    avg_tokens = int(token_stats['avg_tokens'])\n",
        "    max_tokens_in_data = int(token_stats['max_tokens'])\n",
        "    p95_tokens = int(token_stats['p95_tokens'])\n",
        "    exceeds_max = int(token_stats['exceeds_max'])\n",
        "    total_docs = int(token_stats['total_docs'])\n",
        "    truncation_rate = (exceeds_max / total_docs) * 100\n",
        "    \n",
        "    reasons = []\n",
        "    \n",
        "    # Decision Logic\n",
        "    if max_tokens_in_data <= SAFE_TOKEN_THRESHOLD:\n",
        "        strategy = \"EMBED_FULL_TEXT\"\n",
        "        confidence = \"HIGH\"\n",
        "        reasons.append(f\"âœ… Max tokens ({max_tokens_in_data}) well under safe threshold ({SAFE_TOKEN_THRESHOLD})\")\n",
        "        reasons.append(f\"âœ… No risk of truncation\")\n",
        "        reasons.append(f\"âœ… Full semantic meaning preserved for all documents\")\n",
        "        \n",
        "    elif max_tokens_in_data <= MAX_TOKENS:\n",
        "        strategy = \"EMBED_FULL_TEXT\"\n",
        "        confidence = \"MEDIUM\"\n",
        "        reasons.append(f\"âœ… Max tokens ({max_tokens_in_data}) within model limit ({MAX_TOKENS})\")\n",
        "        reasons.append(f\"âš ï¸ Some documents approach the limit - monitor for edge cases\")\n",
        "        reasons.append(f\"âœ… Chunking adds unnecessary complexity for this data\")\n",
        "        \n",
        "    elif truncation_rate < 1.0:\n",
        "        strategy = \"EMBED_FULL_TEXT\"\n",
        "        confidence = \"MEDIUM\"\n",
        "        reasons.append(f\"âš ï¸ {truncation_rate:.2f}% of documents exceed {MAX_TOKENS} tokens\")\n",
        "        reasons.append(f\"âœ… Truncation affects < 1% - acceptable for most use cases\")\n",
        "        reasons.append(f\"ğŸ’¡ Consider: Truncated docs lose tail content; okay if key info is at start\")\n",
        "        \n",
        "    elif truncation_rate < 5.0:\n",
        "        strategy = \"CONSIDER_CHUNKING\"\n",
        "        confidence = \"MEDIUM\"\n",
        "        reasons.append(f\"âš ï¸ {truncation_rate:.2f}% of documents will be truncated\")\n",
        "        reasons.append(f\"âš ï¸ P95 tokens: {p95_tokens} (many docs are long)\")\n",
        "        reasons.append(f\"ğŸ’¡ Evaluate: Is the truncated content important for retrieval?\")\n",
        "        reasons.append(f\"ğŸ’¡ If yes â†’ implement chunking; If no â†’ embed full text\")\n",
        "        \n",
        "    else:\n",
        "        strategy = \"CHUNK_REQUIRED\"\n",
        "        confidence = \"HIGH\"\n",
        "        reasons.append(f\"âŒ {truncation_rate:.2f}% of documents exceed model limit\")\n",
        "        reasons.append(f\"âŒ Significant information loss without chunking\")\n",
        "        reasons.append(f\"ğŸ“‹ Recommended: Chunk at {CHUNK_SIZE} tokens with {CHUNK_OVERLAP} overlap\")\n",
        "        reasons.append(f\"ğŸ“‹ This will create ~{int(avg_tokens / CHUNK_SIZE) + 1} chunks per document on average\")\n",
        "    \n",
        "    return strategy, confidence, reasons\n",
        "\n",
        "\n",
        "# Run decision engine\n",
        "strategy, confidence, reasons = make_decision(stats, token_stats)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ğŸ¯ EMBEDDING STRATEGY RECOMMENDATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n  Strategy:   {strategy}\")\n",
        "print(f\"  Confidence: {confidence}\")\n",
        "print(f\"\\n  Reasoning:\")\n",
        "for reason in reasons:\n",
        "    print(f\"    {reason}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Strategy Comparison\n",
        "\n",
        "| Strategy | When to Use | Pros | Cons |\n",
        "|----------|-------------|------|------|\n",
        "| **EMBED_FULL_TEXT** | Documents < 400 tokens | Simple, complete semantics, fast queries | Limited to short docs |\n",
        "| **EMBED_WITH_TRUNCATION** | Most docs < 512, few exceed | Simple, works for most cases | Loses tail content on long docs |\n",
        "| **CHUNKING** | Documents > 512 tokens | Complete coverage | Complex, more storage, need to merge results |\n",
        "| **HIERARCHICAL** | Very long documents (1000s of tokens) | Best retrieval for long docs | Most complex to implement |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Sample Long Documents (if any)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show longest documents (potential truncation candidates)\n",
        "print(\"Top 5 Longest Documents:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "long_docs_df = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        LENGTH({TEXT_COLUMN}) as char_count,\n",
        "        ROUND(LENGTH({TEXT_COLUMN}) / {CHARS_PER_TOKEN}, 0) as est_tokens,\n",
        "        SUBSTRING({TEXT_COLUMN}, 1, 100) as preview,\n",
        "        SUBSTRING({TEXT_COLUMN}, -100, 100) as tail_preview\n",
        "    FROM {FULL_TABLE_NAME}\n",
        "    WHERE {TEXT_COLUMN} IS NOT NULL\n",
        "    ORDER BY LENGTH({TEXT_COLUMN}) DESC\n",
        "    LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "for i, row in enumerate(long_docs_df.collect(), 1):\n",
        "    print(f\"\\n[{i}] {int(row['char_count'])} chars (~{int(row['est_tokens'])} tokens)\")\n",
        "    print(f\"    Start: \\\"{row['preview']}...\\\"\")\n",
        "    print(f\"    End:   \\\"...{row['tail_preview']}\\\"\")\n",
        "    if row['est_tokens'] > MAX_TOKENS:\n",
        "        lost_tokens = int(row['est_tokens'] - MAX_TOKENS)\n",
        "        print(f\"    âš ï¸ Would lose ~{lost_tokens} tokens if truncated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Implementation Guidance\n",
        "\n",
        "Based on the analysis above, here's how to proceed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate implementation guidance based on decision\n",
        "print(\"=\" * 70)\n",
        "print(\"ğŸ“‹ IMPLEMENTATION STEPS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if strategy == \"EMBED_FULL_TEXT\":\n",
        "    print(\"\"\"\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  EMBED FULL TEXT - Recommended Implementation                       â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  1. Create Vector Search Index in Databricks UI:                    â”‚\n",
        "â”‚     â€¢ Navigate to Catalog â†’ Table â†’ Create Vector Search Index      â”‚\n",
        "â”‚     â€¢ Select \"Delta Sync\" for automatic updates                     â”‚\n",
        "â”‚     â€¢ Choose \"Managed Embeddings\"                                   â”‚\n",
        "â”‚     â€¢ Set embedding column to: {TEXT_COLUMN}                        â”‚\n",
        "â”‚     â€¢ Add filter columns as needed                                  â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  2. No preprocessing required:                                      â”‚\n",
        "â”‚     â€¢ Your text fits within model limits                            â”‚\n",
        "â”‚     â€¢ Full semantic meaning will be preserved                       â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  3. Query directly:                                                 â”‚\n",
        "â”‚     SELECT * FROM VECTOR_SEARCH(                                    â”‚\n",
        "â”‚       index => 'your_catalog.schema.index_name',                    â”‚\n",
        "â”‚       query => 'your search query',                                 â”‚\n",
        "â”‚       num_results => 10                                             â”‚\n",
        "â”‚     )                                                               â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\".format(TEXT_COLUMN=TEXT_COLUMN))\n",
        "\n",
        "elif strategy == \"CONSIDER_CHUNKING\":\n",
        "    print(\"\"\"\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  EVALUATE CHUNKING - Decision Required                              â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  Questions to answer:                                               â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  1. Where is the important information in your documents?           â”‚\n",
        "â”‚     â€¢ Beginning â†’ Full text OK (truncation acceptable)              â”‚\n",
        "â”‚     â€¢ Throughout â†’ Consider chunking                                â”‚\n",
        "â”‚     â€¢ End â†’ Definitely chunk (tail will be lost)                    â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  2. What's the cost of missing information?                         â”‚\n",
        "â”‚     â€¢ Low (e.g., nice-to-have context) â†’ Full text OK               â”‚\n",
        "â”‚     â€¢ High (e.g., key facts anywhere) â†’ Chunk                       â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  3. Query patterns:                                                 â”‚\n",
        "â”‚     â€¢ Users search for concepts at start â†’ Full text OK             â”‚\n",
        "â”‚     â€¢ Users search for any detail â†’ Chunk                           â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  See Section 9 for chunking implementation if needed.               â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\")\n",
        "\n",
        "else:  # CHUNK_REQUIRED\n",
        "    print(\"\"\"\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  CHUNKING REQUIRED - Implementation Steps                           â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  1. Create chunked table:                                           â”‚\n",
        "â”‚     â€¢ See Section 9 for chunking code                               â”‚\n",
        "â”‚     â€¢ Add chunk_id, parent_doc_id columns                           â”‚\n",
        "â”‚     â€¢ Store chunk position for result ordering                      â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  2. Create Vector Search Index on chunked table:                    â”‚\n",
        "â”‚     â€¢ Embedding column: chunk_text                                  â”‚\n",
        "â”‚     â€¢ Include parent_doc_id for grouping results                    â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  3. Query with result aggregation:                                  â”‚\n",
        "â”‚     â€¢ Search returns chunks, not full docs                          â”‚\n",
        "â”‚     â€¢ Group by parent_doc_id to reconstruct context                 â”‚\n",
        "â”‚     â€¢ Consider MMR (Maximal Marginal Relevance) for diversity       â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Chunking Reference Implementation (If Needed)\n",
        "\n",
        "Only use this if the decision engine recommends chunking. This code is provided as a reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# REFERENCE: Chunking implementation (only run if chunking is recommended)\n",
        "# Uncomment and modify as needed\n",
        "\n",
        "\"\"\"\n",
        "from pyspark.sql.functions import col, udf, explode, monotonically_increasing_id\n",
        "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Chunking function\n",
        "def chunk_text(text, chunk_size=400, overlap=50):\n",
        "    '''\n",
        "    Split text into overlapping chunks.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to chunk\n",
        "        chunk_size: Target tokens per chunk (approx)\n",
        "        overlap: Token overlap between chunks\n",
        "    \n",
        "    Returns:\n",
        "        List of (chunk_index, chunk_text) tuples\n",
        "    '''\n",
        "    if not text:\n",
        "        return [(0, \"\")]\n",
        "    \n",
        "    # Approximate: 4 chars per token\n",
        "    char_chunk_size = chunk_size * 4\n",
        "    char_overlap = overlap * 4\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    chunk_idx = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + char_chunk_size\n",
        "        \n",
        "        # Try to break at sentence boundary\n",
        "        if end < len(text):\n",
        "            # Look for sentence end (.!?) within last 20% of chunk\n",
        "            search_start = int(end - char_chunk_size * 0.2)\n",
        "            for i in range(end, search_start, -1):\n",
        "                if text[i] in '.!?' and (i + 1 >= len(text) or text[i + 1] == ' '):\n",
        "                    end = i + 1\n",
        "                    break\n",
        "        \n",
        "        chunk_text = text[start:end].strip()\n",
        "        if chunk_text:\n",
        "            chunks.append((chunk_idx, chunk_text))\n",
        "            chunk_idx += 1\n",
        "        \n",
        "        start = end - char_overlap\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Register UDF\n",
        "chunk_schema = ArrayType(StructType([\n",
        "    StructField(\"chunk_idx\", IntegerType()),\n",
        "    StructField(\"chunk_text\", StringType())\n",
        "]))\n",
        "chunk_udf = udf(lambda text: chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP), chunk_schema)\n",
        "\n",
        "# Create chunked table\n",
        "chunked_df = (\n",
        "    spark.table(FULL_TABLE_NAME)\n",
        "    .withColumn(\"chunks\", chunk_udf(col(TEXT_COLUMN)))\n",
        "    .withColumn(\"chunk\", explode(\"chunks\"))\n",
        "    .select(\n",
        "        col(\"*\"),\n",
        "        col(\"chunk.chunk_idx\").alias(\"chunk_index\"),\n",
        "        col(\"chunk.chunk_text\").alias(\"chunk_text\")\n",
        "    )\n",
        "    .drop(\"chunks\", \"chunk\")\n",
        ")\n",
        "\n",
        "# Save chunked table\n",
        "CHUNKED_TABLE = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}_chunked\"\n",
        "chunked_df.write.mode(\"overwrite\").saveAsTable(CHUNKED_TABLE)\n",
        "\n",
        "print(f\"Created chunked table: {CHUNKED_TABLE}\")\n",
        "print(f\"Original docs: {spark.table(FULL_TABLE_NAME).count()}\")\n",
        "print(f\"Total chunks:  {spark.table(CHUNKED_TABLE).count()}\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"â¬†ï¸ Chunking code is commented out (reference only)\")\n",
        "print(\"   Uncomment and run if the decision engine recommends chunking\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Filter Column Selection\n",
        "\n",
        "## Why Filter Columns Matter\n",
        "\n",
        "Filter columns enable **pre-filtering** before similarity search:\n",
        "- **Query performance**: Reduce search space before vector comparison\n",
        "- **Result relevance**: Only search within relevant subsets\n",
        "- **Cost efficiency**: Less compute for embedding comparisons\n",
        "\n",
        "## Filter Column Decision Framework\n",
        "\n",
        "```\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚  Analyze Each Column            â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                    â”‚\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚  Count Distinct Values          â”‚\n",
        "                    â”‚  (Cardinality)                  â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                    â”‚\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚                           â”‚                           â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   < 20        â”‚           â”‚   20 - 100    â”‚           â”‚    > 100      â”‚\n",
        "â”‚   values      â”‚           â”‚   values      â”‚           â”‚   values      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "        â”‚                           â”‚                           â”‚\n",
        "        â–¼                           â–¼                           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ âœ… EXCELLENT  â”‚           â”‚ âš ï¸ CONSIDER   â”‚           â”‚ âŒ AVOID      â”‚\n",
        "â”‚ Filter Column â”‚           â”‚ If frequently â”‚           â”‚ Use joins or  â”‚\n",
        "â”‚               â”‚           â”‚ filtered on   â”‚           â”‚ post-filter   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Key Principles\n",
        "\n",
        "| Principle | Good Filter | Bad Filter |\n",
        "|-----------|-------------|------------|\n",
        "| **Cardinality** | Low (< 100 distinct values) | High (unique per row) |\n",
        "| **Usage** | Frequently filtered in queries | Rarely or never filtered |\n",
        "| **Selectivity** | Significantly reduces result set | Minimal impact |\n",
        "| **Type** | Categorical, enum-like | Free text, continuous numeric |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Column Cardinality Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COLUMN CARDINALITY ANALYSIS\n",
        "# =============================================================================\n",
        "# Analyze all columns to determine suitability as filter columns\n",
        "\n",
        "from pyspark.sql.types import StringType, IntegerType, BooleanType, DoubleType\n",
        "\n",
        "# Get table schema\n",
        "schema = spark.table(FULL_TABLE_NAME).schema\n",
        "total_rows = spark.sql(f\"SELECT COUNT(*) as cnt FROM {FULL_TABLE_NAME}\").collect()[0]['cnt']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COLUMN CARDINALITY ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTable: {FULL_TABLE_NAME}\")\n",
        "print(f\"Total Rows: {total_rows:,}\")\n",
        "print(\"\\nAnalyzing columns...\\n\")\n",
        "\n",
        "# Analyze each column\n",
        "column_analysis = []\n",
        "\n",
        "for field in schema:\n",
        "    col_name = field.name\n",
        "    col_type = str(field.dataType)\n",
        "    \n",
        "    # Skip the embedding column and primary key (we know these already)\n",
        "    is_embedding = col_name == TEXT_COLUMN\n",
        "    \n",
        "    # Get distinct count\n",
        "    distinct_count = spark.sql(f\"\"\"\n",
        "        SELECT COUNT(DISTINCT `{col_name}`) as distinct_count \n",
        "        FROM {FULL_TABLE_NAME}\n",
        "    \"\"\").collect()[0]['distinct_count']\n",
        "    \n",
        "    # Calculate cardinality ratio\n",
        "    cardinality_ratio = distinct_count / total_rows if total_rows > 0 else 0\n",
        "    \n",
        "    # Determine recommendation\n",
        "    if is_embedding:\n",
        "        recommendation = \"EMBEDDING\"\n",
        "        reason = \"This is the embedding source column\"\n",
        "    elif distinct_count == total_rows:\n",
        "        recommendation = \"âŒ AVOID\"\n",
        "        reason = \"Unique per row (likely PK or ID)\"\n",
        "    elif distinct_count <= 10:\n",
        "        recommendation = \"âœ… EXCELLENT\"\n",
        "        reason = \"Very low cardinality, ideal for filtering\"\n",
        "    elif distinct_count <= 50:\n",
        "        recommendation = \"âœ… GOOD\"\n",
        "        reason = \"Low cardinality, good filter candidate\"\n",
        "    elif distinct_count <= 100:\n",
        "        recommendation = \"âš ï¸ CONSIDER\"\n",
        "        reason = \"Moderate cardinality, use if frequently queried\"\n",
        "    elif cardinality_ratio > 0.5:\n",
        "        recommendation = \"âŒ AVOID\"\n",
        "        reason = \"High cardinality (>50% unique)\"\n",
        "    else:\n",
        "        recommendation = \"âš ï¸ MAYBE\"\n",
        "        reason = f\"Many values ({distinct_count}), evaluate usage patterns\"\n",
        "    \n",
        "    column_analysis.append({\n",
        "        'column': col_name,\n",
        "        'type': col_type,\n",
        "        'distinct': distinct_count,\n",
        "        'ratio': cardinality_ratio,\n",
        "        'recommendation': recommendation,\n",
        "        'reason': reason\n",
        "    })\n",
        "\n",
        "# Sort by distinct count (lowest first = best filters)\n",
        "column_analysis.sort(key=lambda x: (\n",
        "    0 if x['recommendation'].startswith('âœ…') else \n",
        "    1 if x['recommendation'].startswith('âš ï¸') else 2,\n",
        "    x['distinct']\n",
        "))\n",
        "\n",
        "# Display results\n",
        "print(f\"{'Column':<25} {'Type':<15} {'Distinct':>10} {'Ratio':>8} {'Recommendation':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for col in column_analysis:\n",
        "    ratio_str = f\"{col['ratio']:.1%}\" if col['ratio'] < 1 else \"100%\"\n",
        "    print(f\"{col['column']:<25} {col['type'][:14]:<15} {col['distinct']:>10,} {ratio_str:>8} {col['recommendation']:<15}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Filter Column Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FILTER COLUMN RECOMMENDATIONS\n",
        "# =============================================================================\n",
        "\n",
        "# Categorize columns\n",
        "excellent_filters = [c for c in column_analysis if c['recommendation'] == 'âœ… EXCELLENT']\n",
        "good_filters = [c for c in column_analysis if c['recommendation'] == 'âœ… GOOD']\n",
        "consider_filters = [c for c in column_analysis if c['recommendation'].startswith('âš ï¸')]\n",
        "avoid_filters = [c for c in column_analysis if c['recommendation'] == 'âŒ AVOID']\n",
        "embedding_col = [c for c in column_analysis if c['recommendation'] == 'EMBEDDING']\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ğŸ¯ FILTER COLUMN RECOMMENDATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Excellent filters\n",
        "if excellent_filters:\n",
        "    print(\"\\nâœ… EXCELLENT Filter Columns (use these!):\")\n",
        "    print(\"-\" * 50)\n",
        "    for col in excellent_filters:\n",
        "        print(f\"   â€¢ {col['column']:<25} ({col['distinct']} values)\")\n",
        "        print(f\"     {col['reason']}\")\n",
        "\n",
        "# Good filters\n",
        "if good_filters:\n",
        "    print(\"\\nâœ… GOOD Filter Columns (recommended):\")\n",
        "    print(\"-\" * 50)\n",
        "    for col in good_filters:\n",
        "        print(f\"   â€¢ {col['column']:<25} ({col['distinct']} values)\")\n",
        "        print(f\"     {col['reason']}\")\n",
        "\n",
        "# Consider\n",
        "if consider_filters:\n",
        "    print(\"\\nâš ï¸ CONSIDER These (if frequently used in queries):\")\n",
        "    print(\"-\" * 50)\n",
        "    for col in consider_filters:\n",
        "        print(f\"   â€¢ {col['column']:<25} ({col['distinct']} values)\")\n",
        "        print(f\"     {col['reason']}\")\n",
        "\n",
        "# Avoid\n",
        "if avoid_filters:\n",
        "    print(\"\\nâŒ AVOID as Filter Columns:\")\n",
        "    print(\"-\" * 50)\n",
        "    for col in avoid_filters[:5]:  # Show first 5\n",
        "        print(f\"   â€¢ {col['column']:<25} ({col['distinct']} values)\")\n",
        "        print(f\"     {col['reason']}\")\n",
        "    if len(avoid_filters) > 5:\n",
        "        print(f\"   ... and {len(avoid_filters) - 5} more high-cardinality columns\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Sample Value Distribution (Top Filter Candidates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SHOW VALUE DISTRIBUTION FOR TOP FILTER CANDIDATES\n",
        "# =============================================================================\n",
        "\n",
        "# Get top filter candidates (excellent + good)\n",
        "top_candidates = excellent_filters + good_filters\n",
        "\n",
        "if top_candidates:\n",
        "    print(\"VALUE DISTRIBUTION FOR RECOMMENDED FILTER COLUMNS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for col_info in top_candidates[:6]:  # Show up to 6 columns\n",
        "        col_name = col_info['column']\n",
        "        print(f\"\\nğŸ“Š {col_name} ({col_info['distinct']} distinct values):\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Get value distribution\n",
        "        dist_df = spark.sql(f\"\"\"\n",
        "            SELECT \n",
        "                `{col_name}` as value,\n",
        "                COUNT(*) as count,\n",
        "                ROUND(COUNT(*) * 100.0 / {total_rows}, 1) as pct\n",
        "            FROM {FULL_TABLE_NAME}\n",
        "            GROUP BY `{col_name}`\n",
        "            ORDER BY count DESC\n",
        "            LIMIT 10\n",
        "        \"\"\")\n",
        "        \n",
        "        for row in dist_df.collect():\n",
        "            value = row['value'] if row['value'] is not None else \"(NULL)\"\n",
        "            bar_len = int(row['pct'] / 2)\n",
        "            bar = \"â–ˆ\" * bar_len\n",
        "            print(f\"   {str(value):<20} | {row['count']:>6,} ({row['pct']:>5.1f}%) {bar}\")\n",
        "        \n",
        "        if col_info['distinct'] > 10:\n",
        "            print(f\"   ... and {col_info['distinct'] - 10} more values\")\n",
        "else:\n",
        "    print(\"No excellent or good filter candidates found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Generated Filter Column Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GENERATE FILTER COLUMN CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Build recommended filter columns list\n",
        "recommended_filters = []\n",
        "\n",
        "# Add all excellent filters\n",
        "for col in excellent_filters:\n",
        "    recommended_filters.append(col['column'])\n",
        "\n",
        "# Add good filters (up to a reasonable total)\n",
        "for col in good_filters:\n",
        "    if len(recommended_filters) < 8:  # Keep it manageable\n",
        "        recommended_filters.append(col['column'])\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ğŸ“‹ RECOMMENDED FILTER COLUMNS CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nRecommended filter columns ({len(recommended_filters)}):\")\n",
        "print(\"-\" * 50)\n",
        "for i, col in enumerate(recommended_filters, 1):\n",
        "    col_info = next(c for c in column_analysis if c['column'] == col)\n",
        "    print(f\"  {i}. {col:<25} ({col_info['distinct']} values)\")\n",
        "\n",
        "# Generate code snippet\n",
        "filter_list = \", \".join([f'\"{c}\"' for c in recommended_filters])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ“ COPY-PASTE CODE FOR NOTEBOOK 04\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\"\"\n",
        "# Filter columns for Vector Search index\n",
        "FILTER_COLUMNS = [{filter_list}]\n",
        "\n",
        "# Or as comma-separated string for widget:\n",
        "# \"{','.join(recommended_filters)}\"\n",
        "\"\"\")\n",
        "\n",
        "# Store for use in summary\n",
        "RECOMMENDED_FILTER_COLUMNS = recommended_filters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Complete Configuration Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE VECTOR SEARCH CONFIGURATION SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ¯ COMPLETE VECTOR SEARCH CONFIGURATION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  TABLE INFORMATION                                                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Table:              {FULL_TABLE_NAME:<50} â”‚\n",
        "â”‚  Total Rows:         {total_rows:>10,}                                           â”‚\n",
        "â”‚  Total Columns:      {len(schema):>10}                                           â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  PART 1: EMBEDDING STRATEGY                                                 â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Decision:           {strategy:<50} â”‚\n",
        "â”‚  Confidence:         {confidence:<50} â”‚\n",
        "â”‚  Embedding Column:   {TEXT_COLUMN:<50} â”‚\n",
        "â”‚  Embedding Model:    {EMBEDDING_MODEL:<50} â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  PART 2: FILTER COLUMNS                                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Recommended Filters: {len(RECOMMENDED_FILTER_COLUMNS):<49} â”‚\n",
        "â”‚  Columns:            {', '.join(RECOMMENDED_FILTER_COLUMNS[:4]):<50} â”‚\n",
        "â”‚                      {', '.join(RECOMMENDED_FILTER_COLUMNS[4:8]) if len(RECOMMENDED_FILTER_COLUMNS) > 4 else '':<50} â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\")\n",
        "\n",
        "# Print the configuration for notebook 04\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ“‹ READY-TO-USE CONFIGURATION FOR NOTEBOOK 04\")\n",
        "print(\"=\" * 80)\n",
        "print(f'''\n",
        "# Vector Search Index Configuration\n",
        "CATALOG = \"{CATALOG}\"\n",
        "SCHEMA = \"{SCHEMA}\"\n",
        "SOURCE_TABLE = \"{TABLE_NAME}\"\n",
        "INDEX_NAME = \"{TABLE_NAME}_idx\"\n",
        "EMBEDDING_COLUMN = \"{TEXT_COLUMN}\"\n",
        "EMBEDDING_MODEL = \"{EMBEDDING_MODEL}\"\n",
        "FILTER_COLUMNS = {RECOMMENDED_FILTER_COLUMNS}\n",
        "\n",
        "# Widget parameter string:\n",
        "# filter_columns = \"{','.join(RECOMMENDED_FILTER_COLUMNS)}\"\n",
        "''')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"âœ… Analysis complete! Use this configuration in notebook 04-vector-search-create.ipynb\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"=\" * 70)\n",
        "print(\"ğŸ“Š VECTOR SEARCH IMPLEMENTATION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  DATA ANALYZED                                                      â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Table:           {FULL_TABLE_NAME:<40}      â”‚\n",
        "â”‚  Text Column:     {TEXT_COLUMN:<40}      â”‚\n",
        "â”‚  Total Documents: {int(token_stats['total_docs']):>10,}                                   â”‚\n",
        "â”‚  Embedding Model: {EMBEDDING_MODEL:<40}      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  TEXT STATISTICS                                                    â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Avg Characters:  {int(stats['avg_chars']):>10,}                                   â”‚\n",
        "â”‚  Max Characters:  {int(stats['max_chars']):>10,}                                   â”‚\n",
        "â”‚  Avg Tokens:      {int(token_stats['avg_tokens']):>10,} (estimated)                      â”‚\n",
        "â”‚  Max Tokens:      {int(token_stats['max_tokens']):>10,} (estimated)                      â”‚\n",
        "â”‚  Model Limit:     {MAX_TOKENS:>10,}                                   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  RECOMMENDATION                                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Strategy:        {strategy:<40}      â”‚\n",
        "â”‚  Confidence:      {confidence:<40}      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\")\n",
        "\n",
        "# Quick decision summary\n",
        "if strategy == \"EMBED_FULL_TEXT\":\n",
        "    print(\"âœ… VERDICT: Embed the full text. No chunking needed.\")\n",
        "    print(\"   Your documents are short enough that full semantic meaning is preserved.\")\n",
        "elif strategy == \"CONSIDER_CHUNKING\":\n",
        "    print(\"âš ï¸ VERDICT: Evaluate your specific use case.\")\n",
        "    print(\"   Some documents may be truncated. Decide based on where important info lives.\")\n",
        "else:\n",
        "    print(\"ğŸ“‹ VERDICT: Implement chunking before creating the index.\")\n",
        "    print(\"   Too many documents exceed the model limit for full-text embedding.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
