{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870e3b6d-ba07-43e5-961b-cadfa2140afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fashion Retail Gold Layer Data Generation\n",
    "\n",
    "This notebook orchestrates the execution of the Fashion Retail data generation pipeline\n",
    "using the new Python package structure.\n",
    "\n",
    "## Prerequisites:\n",
    "1. The `src/fashion_retail/` package structure should be available\n",
    "2. All dependencies are installed via requirements or pip install\n",
    "\n",
    "## Key Improvements:\n",
    "- ‚úÖ Clean Python package imports (no more complex importlib.util)\n",
    "- ‚úÖ Proper module organization with src/ structure\n",
    "- ‚úÖ Configuration management via dataclass\n",
    "- ‚úÖ Inventory alignment features (Feature: 001-i-want-to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48fd11bd-299f-4ba7-953a-682226d6b398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade faker\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4050412-dccc-489a-8ba8-ff0a11d63da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup: Import the Fashion Retail Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535dd2e6-2371-4d97-a860-d076a210823f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to Python path for clean imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Now we can use clean, standard Python imports\n",
    "from fashion_retail import FashionRetailDataGenerator\n",
    "from fashion_retail.config import load_config\n",
    "from fashion_retail.data import DimensionGenerator, FactGenerator, AggregateGenerator\n",
    "from fashion_retail.inventory import InventoryManager, SalesValidator, StockoutGenerator\n",
    "\n",
    "print(\"‚úÖ Fashion Retail package imported successfully!\")\n",
    "print(\"Available components:\")\n",
    "print(\"  - FashionRetailDataGenerator (main orchestrator)\")\n",
    "print(\"  - Configuration via YAML (load_config)\")\n",
    "print(\"  - Data generators (DimensionGenerator, FactGenerator, AggregateGenerator)\")\n",
    "print(\"  - Inventory alignment (InventoryManager, SalesValidator, StockoutGenerator)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "017d0082-6006-4338-ada1-f9aee8be2853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f6efb5a-e71f-473a-8244-a32fcba24c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: Load default configuration from config.yaml\n",
    "config = load_config()\n",
    "print(\"Default Configuration (from config.yaml):\")\n",
    "print(f\"  Catalog: {config.catalog}\")\n",
    "print(f\"  Schema: {config.schema}\")\n",
    "print(f\"  Customers: {config.customers:,}\")\n",
    "print(f\"  Products: {config.products:,}\")\n",
    "print(f\"  Locations: {config.locations}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Option 2: Load small configuration preset\n",
    "# TIP: You can also just: cp config.small.yaml config.yaml\n",
    "small_config = load_config(\"config.small.yaml\")\n",
    "print(\"Small Configuration (from config.small.yaml):\")\n",
    "print(f\"  Catalog: {small_config.catalog}\")\n",
    "print(f\"  Schema: {small_config.schema}\")\n",
    "print(f\"  Customers: {small_config.customers:,}\")\n",
    "print(f\"  Products: {small_config.products:,}\")\n",
    "print(f\"  Locations: {small_config.locations}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Option 3: Load config with overrides\n",
    "custom_config = load_config(\n",
    "    catalog=\"my_catalog\",\n",
    "    schema=\"my_schema\",\n",
    "    customers=25_000,\n",
    "    products=1_000\n",
    ")\n",
    "print(\"Custom Configuration (config.yaml + overrides):\")\n",
    "print(f\"  Catalog: {custom_config.catalog}\")\n",
    "print(f\"  Schema: {custom_config.schema}\")\n",
    "print(f\"  Customers: {custom_config.customers:,}\")\n",
    "print(f\"  Products: {custom_config.products:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb5fda57-1c43-4948-8757-fc21c27c6226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Choose Your Configuration\n",
    "\n",
    "Select which configuration to use for data generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2ede31-853a-4281-970f-a5a7b35ea768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choose your configuration here:\n",
    "# - Edit config.yaml directly for persistent changes\n",
    "# - Use small_config for quick testing (from config.small.yaml)\n",
    "# - Use config for full scale (from config.yaml)\n",
    "# - TIP: cp config.small.yaml config.yaml to make small config the default\n",
    "\n",
    "selected_config = small_config  # Change this to your preferred config\n",
    "\n",
    "print(f\"Selected configuration: {selected_config.customers:,} customers, {selected_config.products:,} products\")\n",
    "print(f\"Target: {selected_config.full_schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2fb1ad3-7232-4c34-9dcd-b55898e9f8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Execute the Complete Pipeline\n",
    "\n",
    "This runs the full data generation pipeline with inventory alignment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ad724d-fd4e-4c21-8e0c-17521c019325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the generator with current Spark session\n",
    "generator = FashionRetailDataGenerator(spark, selected_config)\n",
    "\n",
    "# Optional: Define a progress callback for visual feedback\n",
    "def progress_callback(stage_name: str, progress_pct: float):\n",
    "    \"\"\"Callback to display progress during pipeline execution.\"\"\"\n",
    "    bar_length = 40\n",
    "    filled = int(bar_length * progress_pct)\n",
    "    bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)\n",
    "    print(f\"\\r[{bar}] {progress_pct*100:5.1f}% - {stage_name}\", end='', flush=True)\n",
    "    if progress_pct >= 1.0:\n",
    "        print()  # New line when complete\n",
    "\n",
    "# Run the complete pipeline\n",
    "try:\n",
    "    print(\"Starting Fashion Retail Data Generation Pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # This will execute all steps:\n",
    "    # 1. Setup catalog/schema\n",
    "    # 2. Create dimensions (customers, products, locations, dates, etc.)\n",
    "    # 3. Create facts with inventory alignment (sales, inventory, events, etc.)\n",
    "    # 4. Create aggregates (affinity scores, size bridge, etc.)\n",
    "    # 5. Enable CDC and optimization\n",
    "    # 6. Run validation\n",
    "    \n",
    "    # NEW: Pipeline now supports progress callbacks and checkpoint resumption\n",
    "    # - progress_callback: Visual progress indicator\n",
    "    # - resume_from_checkpoint: Set to True to resume from last successful stage\n",
    "    generator.run(\n",
    "        progress_callback=progress_callback,\n",
    "        resume_from_checkpoint=False  # Set to True to resume a failed run\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Pipeline completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline failed: {str(e)}\")\n",
    "    print(\"üí° Tip: Re-run with resume_from_checkpoint=True to continue from last successful stage\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7808a765-28a5-43b5-874d-44e09e365dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Alternative: Step-by-Step Execution\n",
    "\n",
    "If you prefer to run each step individually for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5107dad6-5c88-4bc2-990e-9bb981b7e6bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment and run these cells individually if you want step-by-step control:\n",
    "\n",
    "# # Step 1: Setup Catalog and Schema\n",
    "# generator.setup_catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b774bff-b3fc-4c0f-b083-77aa36b1f36d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Step 2: Drop existing tables if needed\n",
    "# if selected_config.force_recreate:\n",
    "#     generator.drop_existing_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43dec4da-693e-4439-b2f6-09d2b9c79f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Step 3: Create Dimensions\n",
    "# generator.create_dimensions()\n",
    "# print(\"‚úÖ All dimensions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb718111-7908-45f6-ae26-f9f7f01db2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Step 4: Create Facts (with inventory alignment)\n",
    "# generator.create_facts()\n",
    "# print(\"‚úÖ All facts created with inventory alignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b9ac8c9-4d1f-444e-8733-093889d78f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Step 5: Create Aggregates\n",
    "# generator.create_bridge_aggregates()\n",
    "# print(\"‚úÖ All aggregates created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9366dda1-d144-4e73-b307-320a13cd9d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Step 6: Apply Optimizations\n",
    "# generator.optimize_tables()\n",
    "# generator.enable_cdc()\n",
    "# print(\"‚úÖ Optimizations applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e117a436-ec59-41e9-b4ce-14edbc359274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validate the Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd7c4ca-483a-413c-82a2-dcb59f1028e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run validation to confirm data volumes match expectations\n",
    "validation_results = {}\n",
    "\n",
    "# Check actual row counts\n",
    "tables_to_validate = [\n",
    "    ('gold_customer_dim', 'customer_key', selected_config.customers),\n",
    "    ('gold_product_dim', 'product_key', selected_config.products),\n",
    "    ('gold_location_dim', 'location_key', selected_config.locations),\n",
    "    ('gold_sales_fact', 'transaction_id', None),  # Variable based on events\n",
    "    ('gold_inventory_fact', 'product_key, location_key, date_key', None),  # Variable\n",
    "    ('gold_customer_product_affinity_agg', 'customer_key, product_key', None)  # Variable\n",
    "]\n",
    "\n",
    "for table, key_cols, expected in tables_to_validate:\n",
    "    try:\n",
    "        actual_count = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as cnt \n",
    "            FROM {selected_config.full_schema_name}.{table}\n",
    "        \"\"\").collect()[0]['cnt']\n",
    "        \n",
    "        # Convert expected to string to avoid mixed types in pandas DataFrame\n",
    "        expected_str = str(expected) if expected is not None else 'Variable'\n",
    "        \n",
    "        validation_results[table] = {\n",
    "            'expected': expected_str,\n",
    "            'actual': str(actual_count),  # Convert to string to ensure consistent types\n",
    "            'status': '‚úÖ' if expected is None or abs(actual_count - expected) / max(expected, 1) < 0.2 else '‚ö†Ô∏è'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        validation_results[table] = {\n",
    "            'expected': 'Variable',\n",
    "            'actual': f'Error: {str(e)}',\n",
    "            'status': '‚ùå'\n",
    "        }\n",
    "\n",
    "# Display results using print instead of display to avoid Arrow conversion issues\n",
    "print(\"üìä Data Validation Results:\")\n",
    "print(\"=\" * 80)\n",
    "for table_name, results in validation_results.items():\n",
    "    print(f\"{results['status']} {table_name}\")\n",
    "    print(f\"   Expected: {results['expected']}\")\n",
    "    print(f\"   Actual:   {results['actual']}\")\n",
    "    print()\n",
    "\n",
    "# Alternative: Create a simple summary table that's safe to display\n",
    "summary_data = []\n",
    "for table_name, results in validation_results.items():\n",
    "    summary_data.append({\n",
    "        'table': table_name,\n",
    "        'expected': results['expected'],\n",
    "        'actual': results['actual'],\n",
    "        'status': results['status']\n",
    "    })\n",
    "\n",
    "# Create Spark DataFrame instead of pandas to avoid Arrow conversion issues\n",
    "validation_df = spark.createDataFrame(summary_data)\n",
    "validation_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "129c7cf5-c4fd-4057-9e19-a8b5fd383ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Inventory Alignment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af56347d-3d87-4fec-bdb8-42aec68fa0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 1: Stockout Rate - Should be around 7.5% (5-10% range)\n",
    "test_stockout_rate = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        'Stockout Rate' as metric,\n",
    "        COUNT(*) as total_positions,\n",
    "        SUM(CASE WHEN is_stockout = TRUE THEN 1 ELSE 0 END) as stockout_positions,\n",
    "        ROUND(SUM(CASE WHEN is_stockout = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as stockout_rate_pct\n",
    "    FROM {selected_config.full_schema_name}.gold_inventory_fact\n",
    "    WHERE date_key = (SELECT MAX(date_key) FROM {selected_config.full_schema_name}.gold_inventory_fact)\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Stockout Rate Analysis:\")\n",
    "display(test_stockout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f43b6b4-b63b-4925-9a52-ef87a0137aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 2: Inventory Constrained Sales - Check how many sales were constrained\n",
    "test_constrained_sales = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        'Inventory Constrained Sales' as metric,\n",
    "        COUNT(*) as total_sales,\n",
    "        SUM(CASE WHEN is_inventory_constrained = TRUE THEN 1 ELSE 0 END) as constrained_sales,\n",
    "        ROUND(SUM(CASE WHEN is_inventory_constrained = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as constrained_pct,\n",
    "        SUM(quantity_requested) as total_requested,\n",
    "        SUM(quantity_sold) as total_sold,\n",
    "        SUM(quantity_requested - quantity_sold) as lost_quantity\n",
    "    FROM {selected_config.full_schema_name}.gold_sales_fact\n",
    "    WHERE quantity_requested IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Inventory Constrained Sales Analysis:\")\n",
    "display(test_constrained_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ed4fee-de3b-4abb-8e63-975dcc1912d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 3: Stockout Events - New table validation\n",
    "test_stockout_events = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        'Stockout Events' as metric,\n",
    "        COUNT(*) as total_events,\n",
    "        SUM(lost_sales_attempts) as total_lost_attempts,\n",
    "        SUM(lost_sales_quantity) as total_lost_quantity,\n",
    "        ROUND(SUM(lost_sales_revenue), 2) as total_lost_revenue,\n",
    "        SUM(CASE WHEN peak_season_flag = TRUE THEN 1 ELSE 0 END) as peak_season_stockouts,\n",
    "        ROUND(AVG(stockout_duration_days), 1) as avg_duration_days\n",
    "    FROM {selected_config.full_schema_name}.gold_stockout_events\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Stockout Events Analysis:\")\n",
    "display(test_stockout_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52539a1c-def8-42fa-a573-8c4b2fe27005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 4: Cart Abandonment - Low Inventory Impact\n",
    "test_low_inventory_abandonment = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        'Low Inventory Cart Abandonment' as metric,\n",
    "        COUNT(*) as total_abandonments,\n",
    "        SUM(CASE WHEN low_inventory_trigger = TRUE THEN 1 ELSE 0 END) as low_inventory_abandonments,\n",
    "        ROUND(SUM(CASE WHEN low_inventory_trigger = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as low_inv_pct,\n",
    "        AVG(inventory_constrained_items) as avg_constrained_items\n",
    "    FROM {selected_config.full_schema_name}.gold_cart_abandonment_fact\n",
    "    WHERE low_inventory_trigger IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Cart Abandonment Analysis:\")\n",
    "display(test_low_inventory_abandonment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62820e7e-a2e5-48e7-9b6e-9f203a128f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 5: No Negative Inventory - Critical validation\n",
    "test_no_negative_inventory = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        'Negative Inventory Violations' as metric,\n",
    "        COUNT(*) as violation_count,\n",
    "        CASE\n",
    "            WHEN COUNT(*) = 0 THEN '‚úÖ PASS'\n",
    "            ELSE '‚ùå FAIL'\n",
    "        END as test_result\n",
    "    FROM {selected_config.full_schema_name}.gold_inventory_fact\n",
    "    WHERE quantity_available < 0\n",
    "\"\"\")\n",
    "\n",
    "print(\"üîç Data Integrity Check:\")\n",
    "display(test_no_negative_inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b9f3bc4-599a-4572-a88e-c02c3f5f35f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Analytics Queries\n",
    "\n",
    "Test the key use cases with the generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bbf8075-9298-448a-8c34-bd0a6cd874c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Case 1: Real-time Personalization\n",
    "print(\"üéØ Use Case 1: Real-time Personalization\")\n",
    "\n",
    "personalization_sample = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        'Personalization Data' as use_case,\n",
    "        COUNT(DISTINCT customer_key) as customers_with_affinity,\n",
    "        AVG(affinity_score) as avg_affinity,\n",
    "        MAX(affinity_score) as max_affinity,\n",
    "        AVG(predicted_cltv_impact) as avg_cltv_impact\n",
    "    FROM {selected_config.full_schema_name}.gold_customer_product_affinity_agg\n",
    "    WHERE affinity_score > 0\n",
    "\"\"\")\n",
    "\n",
    "display(personalization_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a714946-ea5d-4867-a50a-e94c866bce3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Case 2: Inventory Optimization\n",
    "print(\"üì¶ Use Case 2: Inventory Optimization\")\n",
    "\n",
    "inventory_health = spark.sql(f\"\"\"\n",
    "    WITH inventory_summary AS (\n",
    "        SELECT \n",
    "            SUM(CASE WHEN is_stockout THEN 1 ELSE 0 END) as stockout_count,\n",
    "            SUM(CASE WHEN is_overstock THEN 1 ELSE 0 END) as overstock_count,\n",
    "            AVG(days_of_supply) as avg_days_supply,\n",
    "            COUNT(DISTINCT product_key) as products_tracked,\n",
    "            COUNT(DISTINCT location_key) as locations_tracked\n",
    "        FROM {selected_config.full_schema_name}.gold_inventory_fact\n",
    "        WHERE date_key = (SELECT MAX(date_key) FROM {selected_config.full_schema_name}.gold_inventory_fact)\n",
    "    )\n",
    "    SELECT \n",
    "        'Inventory Health' as use_case,\n",
    "        stockout_count,\n",
    "        overstock_count,\n",
    "        ROUND(avg_days_supply, 1) as avg_days_supply,\n",
    "        products_tracked,\n",
    "        locations_tracked\n",
    "    FROM inventory_summary\n",
    "\"\"\")\n",
    "\n",
    "display(inventory_health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58a6085f-4a56-4ed0-917b-15def2041260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Case 3: Demand Forecasting\n",
    "print(\"üìà Use Case 3: Demand Forecasting\")\n",
    "\n",
    "forecast_accuracy = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        'Demand Forecast' as use_case,\n",
    "        COUNT(*) as forecasts_with_actuals,\n",
    "        ROUND(AVG(forecast_accuracy), 1) as avg_accuracy_pct,\n",
    "        ROUND(AVG(mape), 1) as avg_mape,\n",
    "        COUNT(DISTINCT product_key) as products_forecasted\n",
    "    FROM {selected_config.full_schema_name}.gold_demand_forecast_fact\n",
    "    WHERE actual_quantity IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "display(forecast_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a161269-605b-4318-9b8b-f8f7ef7e6b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Generation Complete with New Package Structure!\n",
    "\n",
    "The data has been generated using the **new Python package structure**, providing:\n",
    "\n",
    "### üèóÔ∏è **Improved Architecture:**\n",
    "- ‚úÖ Clean Python package imports (no more complex `importlib.util`)\n",
    "- ‚úÖ Proper `src/fashion_retail/` package organization\n",
    "- ‚úÖ Configuration management via dataclass with validation\n",
    "- ‚úÖ Modular design with clear separation of concerns\n",
    "- ‚úÖ **Progress tracking and checkpointing** for long-running pipelines\n",
    "\n",
    "### üìä **Data Realism Features:**\n",
    "- ‚úÖ **Realistic customer names and emails** (via Faker library)\n",
    "- ‚úÖ **Location-based tax rates** (state-specific sales tax)\n",
    "- ‚úÖ **Accurate margin calculations** (from actual unit cost)\n",
    "- ‚úÖ **Realistic return rates** (2.5x multiplier for digital channels)\n",
    "- ‚úÖ **Inventory-aligned customer behavior** (Feature: 001-i-want-to)\n",
    "  - Sales constrained by available inventory (no phantom sales!)\n",
    "  - 5-10% stockout rate across product-location combinations\n",
    "  - Returns replenish inventory 1-3 days after return date\n",
    "  - Cart abandonment +10pp higher when low inventory detected\n",
    "  - New `gold_stockout_events` table with lost sales analytics\n",
    "\n",
    "### üîß **Technical Improvements:**\n",
    "- **Standard Python Imports**: `from fashion_retail import FashionRetailDataGenerator`\n",
    "- **Configuration Validation**: Automatic validation of all parameters\n",
    "- **Progress Callbacks**: Visual progress tracking during generation\n",
    "- **Checkpoint Resumption**: Resume failed pipelines from last successful stage\n",
    "- **Modular Components**: Separate packages for data generation and inventory management\n",
    "- **Better IDE Support**: Autocomplete, navigation, and debugging\n",
    "- **Easier Testing**: Standard Python package structure for unit tests\n",
    "\n",
    "### üìã **Next Steps:**\n",
    "1. Explore the generated data using the validation queries above\n",
    "2. Build ML models using the inventory-constrained features\n",
    "3. Set up incremental pipelines using the CDC-enabled tables\n",
    "4. Extend the package with additional generators or analytics\n",
    "5. Install Faker for realistic names: `pip install faker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa5aba18-015c-49ed-b15f-99d59c4628ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display final summary\n",
    "print(\"üìã Final Data Summary:\")\n",
    "\n",
    "summary = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        table_name,\n",
    "        num_rows,\n",
    "        last_modified\n",
    "    FROM (\n",
    "        SELECT \n",
    "            'gold_customer_dim' as table_name,\n",
    "            COUNT(*) as num_rows,\n",
    "            MAX(etl_timestamp) as last_modified\n",
    "        FROM {selected_config.full_schema_name}.gold_customer_dim\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'gold_product_dim' as table_name,\n",
    "            COUNT(*) as num_rows,\n",
    "            MAX(etl_timestamp) as last_modified\n",
    "        FROM {selected_config.full_schema_name}.gold_product_dim\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'gold_sales_fact' as table_name,\n",
    "            COUNT(*) as num_rows,\n",
    "            MAX(etl_timestamp) as last_modified\n",
    "        FROM {selected_config.full_schema_name}.gold_sales_fact\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'gold_inventory_fact' as table_name,\n",
    "            COUNT(*) as num_rows,\n",
    "            MAX(etl_timestamp) as last_modified\n",
    "        FROM {selected_config.full_schema_name}.gold_inventory_fact\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'gold_stockout_events' as table_name,\n",
    "            COUNT(*) as num_rows,\n",
    "            MAX(etl_timestamp) as last_modified\n",
    "        FROM {selected_config.full_schema_name}.gold_stockout_events\n",
    "    )\n",
    "    ORDER BY num_rows DESC\n",
    "\"\"\")\n",
    "\n",
    "display(summary)\n",
    "\n",
    "print(f\"\\nüéâ Data generation completed successfully!\")\n",
    "print(f\"üìç Location: {selected_config.full_schema_name}\")\n",
    "print(f\"üèóÔ∏è Architecture: Clean Python package structure\")\n",
    "print(f\"üìä Features: Inventory alignment + realistic business patterns\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-fashion-retail-data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
