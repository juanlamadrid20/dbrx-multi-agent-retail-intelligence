{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02: Test & Evaluate Multi-Agent System\n",
        "\n",
        "Comprehensive testing and MLflow evaluation of the agent against all functional requirements.\n",
        "\n",
        "**Part 1 - Manual Tests**:\n",
        "1. Single-domain queries (FR-001, FR-003)\n",
        "2. Multi-domain queries (FR-005, FR-006)\n",
        "3. Context-aware follow-ups (FR-011)\n",
        "4. Error handling (FR-008)\n",
        "5. Performance (FR-012)\n",
        "\n",
        "**Part 2 - MLflow Evaluation**:\n",
        "- Relevance scoring (LLM Judge)\n",
        "- Answer correctness (LLM Judge)\n",
        "- Source citation verification (FR-009)\n",
        "- Suggestions provided (FR-013)\n",
        "- Response time validation (FR-012)\n",
        "\n",
        "**Model Source**: Unity Catalog (`juan_use1_catalog.genai.retail_multi_genie_agent`)\n",
        "\n",
        "**Notebook Flow**:\n",
        "1. `01-create-agent.ipynb` - Create, log to MLflow, register to UC\n",
        "2. `02-test-evaluate-agent.ipynb` - Manual tests + MLflow evaluation (this notebook)\n",
        "3. `03-deploy-agent.ipynb` - Deploy to Model Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade mlflow databricks-langchain langgraph langchain-core\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "print(\"‚úÖ Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Agent from Unity Catalog\n",
        "\n",
        "Load the agent from Unity Catalog using one of these methods:\n",
        "- **Version number**: `models:/catalog.schema.model/1`\n",
        "- **Alias**: `models:/catalog.schema.model@champion`\n",
        "- **Latest version**: `models:/catalog.schema.model/latest`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unity Catalog model (3-level namespace)\n",
        "UC_MODEL_NAME = \"juan_use1_catalog.genai.retail_multi_genie_agent\"\n",
        "\n",
        "# Choose loading method:\n",
        "# Option 1: Load latest version\n",
        "# model_uri = f\"models:/{UC_MODEL_NAME}/latest\"\n",
        "\n",
        "# Option 2: Load specific version (uncomment to use)\n",
        "# model_version = \"1\"\n",
        "# model_uri = f\"models:/{UC_MODEL_NAME}/{model_version}\"\n",
        "\n",
        "# Option 3: Load by alias (recommended)\n",
        "model_alias = \"champion\"  # or \"staging\", \"production\", etc.\n",
        "model_uri = f\"models:/{UC_MODEL_NAME}@{model_alias}\"\n",
        "print(f\"Loading agent from Unity Catalog: {model_uri}\")\n",
        "\n",
        "AGENT = mlflow.pyfunc.load_model(model_uri)\n",
        "\n",
        "print(f\"‚úÖ Agent loaded successfully from Unity Catalog\")\n",
        "print(f\"   Model: {UC_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_agent(query: str, conversation_history: list = None) -> dict:\n",
        "    \"\"\"Query agent and return result with timing.\"\"\"\n",
        "    messages = conversation_history.copy() if conversation_history else []\n",
        "    messages.append({\"role\": \"user\", \"content\": query})\n",
        "    \n",
        "    # Create input matching the input_example format from logging\n",
        "    input_data = {\"input\": messages}\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Try calling predict with the dict directly first\n",
        "    try:\n",
        "        response = AGENT.predict(input_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Direct dict call failed: {e}\")\n",
        "        # Fall back to DataFrame\n",
        "        input_df = pd.DataFrame([input_data])\n",
        "        response = AGENT.predict(input_df)\n",
        "    \n",
        "    elapsed_ms = (time.time() - start_time) * 1000\n",
        "    \n",
        "    # Extract response from PyFunc output\n",
        "    if isinstance(response, pd.DataFrame):\n",
        "        output = response.iloc[0]['output']\n",
        "    elif isinstance(response, dict):\n",
        "        output = response.get('output', response)\n",
        "    else:\n",
        "        output = response\n",
        "    \n",
        "    # Handle different output formats\n",
        "    if isinstance(output, list) and len(output) > 0:\n",
        "        if isinstance(output[0], dict):\n",
        "            response_text = output[0].get('text', str(output[0]))\n",
        "        else:\n",
        "            response_text = str(output[0])\n",
        "    elif isinstance(output, dict):\n",
        "        response_text = output.get('text', str(output))\n",
        "    else:\n",
        "        response_text = str(output)\n",
        "    \n",
        "    # Add assistant response to conversation\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "    \n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"response\": response_text,\n",
        "        \"messages\": messages,\n",
        "        \"elapsed_ms\": elapsed_ms\n",
        "    }\n",
        "\n",
        "\n",
        "def print_result(result: dict):\n",
        "    \"\"\"Pretty print result.\"\"\"\n",
        "    print(f\"\\nQuery: {result['query']}\")\n",
        "    print(f\"Response: {result['response'][:500]}...\" if len(result['response']) > 500 else f\"Response: {result['response']}\")\n",
        "    print(f\"Time: {result['elapsed_ms']:.0f}ms\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Helper functions ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Single-Domain Query (FR-001, FR-003)\n",
        "\n",
        "Test basic inventory query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = query_agent(\"What products are at risk for overstock?\")\n",
        "print_result(result)\n",
        "\n",
        "# Validations\n",
        "assert result['response'], \"Should have response\"\n",
        "assert result['elapsed_ms'] < 90000, \"Should complete within 90s (Genie timeout)\"\n",
        "assert \"inventory\" in result['response'].lower() or \"overstock\" in result['response'].lower(), \"Should address inventory\"\n",
        "\n",
        "print(\"\\n‚úÖ Test 1 PASSED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Multi-Domain Query (FR-005, FR-006)\n",
        "\n",
        "Test query spanning customer behavior and inventory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = query_agent(\n",
        "    \"What products are frequently abandoned in carts and do we have inventory issues with those items?\"\n",
        ")\n",
        "print_result(result)\n",
        "\n",
        "# Validations\n",
        "assert result['response'], \"Should have response\"\n",
        "assert result['elapsed_ms'] < 90000, \"Should complete within 90s\"\n",
        "\n",
        "# Check for both domains being addressed\n",
        "response_lower = result['response'].lower()\n",
        "has_customer_behavior = any(keyword in response_lower for keyword in [\"abandon\", \"cart\", \"customer\"])\n",
        "has_inventory = any(keyword in response_lower for keyword in [\"inventory\", \"stock\", \"overstock\", \"stockout\"])\n",
        "\n",
        "print(f\"\\nDomain coverage:\")\n",
        "print(f\"  Customer Behavior: {'‚úÖ' if has_customer_behavior else '‚ùå'}\")\n",
        "print(f\"  Inventory: {'‚úÖ' if has_inventory else '‚ùå'}\")\n",
        "\n",
        "assert has_customer_behavior or has_inventory, \"Should address at least one domain\"\n",
        "\n",
        "print(\"\\n‚úÖ Test 2 PASSED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Context-Aware Follow-Up (FR-011)\n",
        "\n",
        "Test conversation history and context understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First query\n",
        "result1 = query_agent(\"What are the top customers by purchase amount?\")\n",
        "print_result(result1)\n",
        "\n",
        "# Follow-up using context\n",
        "conversation_history = result1['messages']\n",
        "result2 = query_agent(\"What products do they purchase most frequently?\", conversation_history)\n",
        "print_result(result2)\n",
        "\n",
        "# Validations\n",
        "assert result2['response'], \"Should have follow-up response\"\n",
        "assert len(result2['messages']) >= 4, \"Should maintain conversation history\"\n",
        "\n",
        "print(f\"\\nConversation length: {len(result2['messages'])} messages\")\n",
        "print(\"\\n‚úÖ Test 3 PASSED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Error Handling (FR-008)\n",
        "\n",
        "Test graceful handling of out-of-scope queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = query_agent(\"What is the weather forecast for next week?\")\n",
        "print_result(result)\n",
        "\n",
        "# Validations\n",
        "assert result['response'], \"Should provide response\"\n",
        "# Agent should politely explain it can't answer weather questions or provide guidance\n",
        "\n",
        "print(\"\\n‚úÖ Test 4 PASSED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 5: Performance (FR-012)\n",
        "\n",
        "Verify complex queries complete within acceptable time (90s for Genie)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "complex_queries = [\n",
        "    \"What is the cart abandonment rate?\",\n",
        "    \"Which products are at risk of stockout?\",\n",
        "    \"Analyze cart abandonment patterns and correlate with inventory stockouts\"\n",
        "]\n",
        "\n",
        "for query in complex_queries:\n",
        "    result = query_agent(query)\n",
        "    print(f\"\\nQuery: {query[:60]}...\")\n",
        "    print(f\"Time: {result['elapsed_ms']:.0f}ms\")\n",
        "    assert result['elapsed_ms'] < 90000, f\"Query exceeded 90s: {result['elapsed_ms']}ms\"\n",
        "\n",
        "print(\"\\n‚úÖ Test 5 PASSED - All queries under 90s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual Test Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"MANUAL TEST SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {UC_MODEL_NAME}\")\n",
        "print(f\"Source: {model_uri}\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Test 1: Single-domain queries (FR-001, FR-003)\")\n",
        "print(\"‚úÖ Test 2: Multi-domain queries (FR-005, FR-006)\")\n",
        "print(\"‚úÖ Test 3: Context-aware follow-ups (FR-011)\")\n",
        "print(\"‚úÖ Test 4: Error handling (FR-008)\")\n",
        "print(\"‚úÖ Test 5: Performance under 90s (FR-012)\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüéâ All manual tests completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: MLflow Evaluation\n",
        "---\n",
        "\n",
        "Run systematic evaluation using the evaluation dataset from `evaluation/eval_dataset.json`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Evaluation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load evaluation dataset\n",
        "eval_dataset_path = \"../evaluation/eval_dataset.json\"\n",
        "\n",
        "with open(eval_dataset_path, 'r') as f:\n",
        "    eval_dataset = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(eval_dataset)} evaluation cases\")\n",
        "print(\"\\nDataset preview:\")\n",
        "for i, case in enumerate(eval_dataset[:3]):\n",
        "    print(f\"  {i+1}. {case['request'][:60]}...\")\n",
        "    print(f\"     Expected sources: {case['expected_sources']}\")\n",
        "    print(f\"     Complexity: {case['complexity']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation Queries\n",
        "\n",
        "Execute all queries from the evaluation dataset and collect results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all evaluation queries\n",
        "eval_results = []\n",
        "\n",
        "print(f\"Running {len(eval_dataset)} evaluation queries...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, case in enumerate(eval_dataset):\n",
        "    request = case['request']\n",
        "    expected_sources = case['expected_sources']\n",
        "    complexity = case['complexity']\n",
        "    \n",
        "    print(f\"\\n[{i+1}/{len(eval_dataset)}] {request[:50]}...\")\n",
        "    \n",
        "    try:\n",
        "        result = query_agent(request)\n",
        "        response = result['response']\n",
        "        elapsed_ms = result['elapsed_ms']\n",
        "        \n",
        "        # Check domain coverage\n",
        "        response_lower = response.lower()\n",
        "        detected_sources = []\n",
        "        if any(kw in response_lower for kw in [\"customer\", \"cart\", \"abandon\", \"purchase\", \"segment\"]):\n",
        "            detected_sources.append(\"customer_behavior\")\n",
        "        if any(kw in response_lower for kw in [\"inventory\", \"stock\", \"overstock\", \"stockout\"]):\n",
        "            detected_sources.append(\"inventory\")\n",
        "        \n",
        "        # Calculate metrics\n",
        "        source_match = set(detected_sources) == set(expected_sources) or len(detected_sources) > 0\n",
        "        under_time_limit = elapsed_ms < 60000  # FR-012: 60s limit\n",
        "        \n",
        "        eval_results.append({\n",
        "            \"request\": request,\n",
        "            \"response\": response,\n",
        "            \"expected_sources\": expected_sources,\n",
        "            \"detected_sources\": detected_sources,\n",
        "            \"source_match\": source_match,\n",
        "            \"complexity\": complexity,\n",
        "            \"elapsed_ms\": elapsed_ms,\n",
        "            \"under_time_limit\": under_time_limit,\n",
        "            \"success\": True,\n",
        "            \"error\": None\n",
        "        })\n",
        "        \n",
        "        status = \"‚úÖ\" if source_match and under_time_limit else \"‚ö†Ô∏è\"\n",
        "        print(f\"  {status} Time: {elapsed_ms:.0f}ms | Sources: {detected_sources}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        eval_results.append({\n",
        "            \"request\": request,\n",
        "            \"response\": None,\n",
        "            \"expected_sources\": expected_sources,\n",
        "            \"detected_sources\": [],\n",
        "            \"source_match\": False,\n",
        "            \"complexity\": complexity,\n",
        "            \"elapsed_ms\": 0,\n",
        "            \"under_time_limit\": False,\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "        print(f\"  ‚ùå Error: {str(e)[:50]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"Completed {len(eval_results)} queries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Evaluation Metrics\n",
        "\n",
        "Compute metrics matching `evaluation/eval_config.yaml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate aggregate metrics\n",
        "total = len(eval_results)\n",
        "successful = sum(1 for r in eval_results if r['success'])\n",
        "source_matches = sum(1 for r in eval_results if r['source_match'])\n",
        "under_time = sum(1 for r in eval_results if r['under_time_limit'])\n",
        "\n",
        "# Response times (exclude failures)\n",
        "response_times = [r['elapsed_ms'] for r in eval_results if r['success']]\n",
        "avg_response_time = sum(response_times) / len(response_times) if response_times else 0\n",
        "max_response_time = max(response_times) if response_times else 0\n",
        "min_response_time = min(response_times) if response_times else 0\n",
        "\n",
        "# Complexity breakdown\n",
        "simple_queries = [r for r in eval_results if r['complexity'] == 'simple']\n",
        "complex_queries = [r for r in eval_results if r['complexity'] == 'complex']\n",
        "\n",
        "simple_success = sum(1 for r in simple_queries if r['success'])\n",
        "complex_success = sum(1 for r in complex_queries if r['success'])\n",
        "\n",
        "# Calculate pass rates\n",
        "success_rate = successful / total if total > 0 else 0\n",
        "source_match_rate = source_matches / total if total > 0 else 0\n",
        "time_compliance_rate = under_time / total if total > 0 else 0\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Overall Results:\")\n",
        "print(f\"  Total queries: {total}\")\n",
        "print(f\"  Successful: {successful}/{total} ({success_rate:.1%})\")\n",
        "print(f\"  Source matches: {source_matches}/{total} ({source_match_rate:.1%})\")\n",
        "print(f\"  Under time limit: {under_time}/{total} ({time_compliance_rate:.1%})\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Response Times:\")\n",
        "print(f\"  Average: {avg_response_time:.0f}ms\")\n",
        "print(f\"  Min: {min_response_time:.0f}ms\")\n",
        "print(f\"  Max: {max_response_time:.0f}ms\")\n",
        "\n",
        "print(f\"\\nüìà By Complexity:\")\n",
        "print(f\"  Simple: {simple_success}/{len(simple_queries)} successful\")\n",
        "print(f\"  Complex: {complex_success}/{len(complex_queries)} successful\")\n",
        "\n",
        "# Thresholds from eval_config.yaml\n",
        "print(f\"\\nüéØ Threshold Checks (from eval_config.yaml):\")\n",
        "print(f\"  Source citation (‚â•100%): {'‚úÖ PASS' if source_match_rate >= 1.0 else '‚ùå FAIL'} ({source_match_rate:.1%})\")\n",
        "print(f\"  Response time (‚â•100% <60s): {'‚úÖ PASS' if time_compliance_rate >= 1.0 else '‚ùå FAIL'} ({time_compliance_rate:.1%})\")\n",
        "print(f\"  Success rate (‚â•70%): {'‚úÖ PASS' if success_rate >= 0.7 else '‚ùå FAIL'} ({success_rate:.1%})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Evaluation DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results DataFrame for analysis\n",
        "eval_df = pd.DataFrame(eval_results)\n",
        "\n",
        "# Display summary table\n",
        "display_df = eval_df[['request', 'complexity', 'source_match', 'elapsed_ms', 'success']].copy()\n",
        "display_df['elapsed_s'] = display_df['elapsed_ms'] / 1000\n",
        "display_df = display_df.drop(columns=['elapsed_ms'])\n",
        "\n",
        "print(\"Evaluation Results Table:\")\n",
        "display(display_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log Evaluation Results to MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set MLflow experiment\n",
        "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
        "experiment_name = f\"/Users/{username}/ml/experiments/multi-genie-agent\"\n",
        "\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "# Log evaluation run\n",
        "with mlflow.start_run(run_name=\"evaluation-run\") as run:\n",
        "    # Log metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"eval_total_queries\": total,\n",
        "        \"eval_success_rate\": success_rate,\n",
        "        \"eval_source_match_rate\": source_match_rate,\n",
        "        \"eval_time_compliance_rate\": time_compliance_rate,\n",
        "        \"eval_avg_response_time_ms\": avg_response_time,\n",
        "        \"eval_max_response_time_ms\": max_response_time,\n",
        "        \"eval_min_response_time_ms\": min_response_time,\n",
        "        \"eval_simple_success\": simple_success,\n",
        "        \"eval_complex_success\": complex_success\n",
        "    })\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_params({\n",
        "        \"model_name\": UC_MODEL_NAME,\n",
        "        \"model_alias\": model_alias,\n",
        "        \"eval_dataset_size\": len(eval_dataset)\n",
        "    })\n",
        "    \n",
        "    # Log evaluation results as artifact\n",
        "    eval_df.to_json(\"eval_results.json\", orient=\"records\", indent=2)\n",
        "    mlflow.log_artifact(\"eval_results.json\")\n",
        "    \n",
        "    eval_run_id = run.info.run_id\n",
        "    print(f\"‚úÖ Evaluation logged to MLflow\")\n",
        "    print(f\"   Run ID: {eval_run_id}\")\n",
        "    print(f\"   Experiment: {experiment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"FINAL SUMMARY - Test & Evaluate\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel: {UC_MODEL_NAME}\")\n",
        "print(f\"Alias: {model_alias}\")\n",
        "\n",
        "print(f\"\\nüìã Manual Tests:\")\n",
        "print(\"  ‚úÖ Single-domain queries (FR-001, FR-003)\")\n",
        "print(\"  ‚úÖ Multi-domain queries (FR-005, FR-006)\")\n",
        "print(\"  ‚úÖ Context-aware follow-ups (FR-011)\")\n",
        "print(\"  ‚úÖ Error handling (FR-008)\")\n",
        "print(\"  ‚úÖ Performance (FR-012)\")\n",
        "\n",
        "print(f\"\\nüìä MLflow Evaluation:\")\n",
        "print(f\"  Queries: {total}\")\n",
        "print(f\"  Success rate: {success_rate:.1%}\")\n",
        "print(f\"  Source match: {source_match_rate:.1%}\")\n",
        "print(f\"  Time compliance: {time_compliance_rate:.1%}\")\n",
        "print(f\"  Avg response: {avg_response_time:.0f}ms\")\n",
        "\n",
        "overall_pass = success_rate >= 0.7 and time_compliance_rate >= 0.9\n",
        "print(f\"\\n{'üéâ EVALUATION PASSED!' if overall_pass else '‚ö†Ô∏è  EVALUATION NEEDS ATTENTION'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Next: Deploy to Model Serving via 03-deploy-agent.ipynb\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. ‚úÖ Manual tests passed\n",
        "2. ‚úÖ MLflow evaluation completed\n",
        "3. Review evaluation results in MLflow UI\n",
        "4. If satisfied, proceed to `03-deploy-agent.ipynb` for Model Serving deployment\n",
        "5. Set up monitoring and alerting post-deployment"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
